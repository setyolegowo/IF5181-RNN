{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_lstm_handson.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "a2s_6XuMTGen"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA17VXfOSDjK",
        "colab_type": "text"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK2ugT7IRlcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2s_6XuMTGen",
        "colab_type": "text"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6nns36yTIkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def initialize_adam(parameters) :\n",
        "    \"\"\"\n",
        "    Initializes v and s as two python dictionaries with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
        "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters.\n",
        "                    parameters[\"W\" + str(l)] = Wl\n",
        "                    parameters[\"b\" + str(l)] = bl\n",
        "    \n",
        "    Returns: \n",
        "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
        "                    v[\"dW\" + str(l)] = ...\n",
        "                    v[\"db\" + str(l)] = ...\n",
        "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
        "                    s[\"dW\" + str(l)] = ...\n",
        "                    s[\"db\" + str(l)] = ...\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
        "    for l in range(L):\n",
        "    ### START CODE HERE ### (approx. 4 lines)\n",
        "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
        "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
        "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
        "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return v, s\n",
        "\n",
        "\n",
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
        "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
        "    \"\"\"\n",
        "    Update parameters using Adam\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    learning_rate -- the learning rate, scalar.\n",
        "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
        "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
        "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "    \n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(L):\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)] \n",
        "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)] \n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1**t)\n",
        "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1**t)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (grads[\"dW\" + str(l+1)] ** 2)\n",
        "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (grads[\"db\" + str(l+1)] ** 2)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t)\n",
        "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / np.sqrt(s_corrected[\"dW\" + str(l+1)] + epsilon)\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / np.sqrt(s_corrected[\"db\" + str(l+1)] + epsilon)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    return parameters, v, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P852k-YnSGO2",
        "colab_type": "text"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8-hD29jusP3",
        "colab_type": "text"
      },
      "source": [
        "### Forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl8ikwVCSGmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_cell_forward(xt, a_prev, parameters):\n",
        "  # Retrieve parameters from \"parameters\"\n",
        "  (Wax, Waa, Wya, ba, by) = (parameters[\"Wax\"], parameters[\"Waa\"], parameters[\"Wya\"], parameters[\"ba\"], parameters[\"by\"])\n",
        "\n",
        "  # Compute next activation state using the formula given above\n",
        "  a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n",
        "\n",
        "  # Compute output of the current cell using the formula given above\n",
        "  yt_pred = softmax(np.dot(Wya, a_next) + by)\n",
        "\n",
        "  # Store values you need for backward propagation in cache\n",
        "  cache = (a_next, a_prev, xt, parameters)\n",
        "\n",
        "  return a_next, yt_pred, cache\n",
        "\n",
        "def rnn_forward(x, a0, parameters):\n",
        "  # Initialize \"caches\" which will contain the list of all caches\n",
        "  caches = []\n",
        "\n",
        "  # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n",
        "  n_x, m, T_x = x.shape\n",
        "  n_y, n_a = parameters[\"Wya\"].shape\n",
        "\n",
        "  # Initialize \"a\" and \"y\" with zeros (=2 lines)\n",
        "  (a, y_pred) = (np.zeros((n_a, m, T_x)), np.zeros((n_y, m, T_x)))\n",
        "\n",
        "  # Initialize a_next (=1 line)\n",
        "  a_next = a0\n",
        "\n",
        "  # Loop over all time-steps\n",
        "  for t in range(0, T_x):\n",
        "    # Update next hiddent state, compute the prediction, get the cache(=1 line)\n",
        "    a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)\n",
        "\n",
        "    # Save the value of the new \"next\" hidden state in a (=1 line)\n",
        "    a[:,:,t] = a_next\n",
        "\n",
        "    # Save the value of the prediction in y (=1 line)\n",
        "    y_pred[:,:,t] = yt_pred\n",
        "\n",
        "    # Append \"cache\" to \"caches\" (=1 line)\n",
        "    caches.append(cache)\n",
        "  \n",
        "  # Store values needed for backward propagation in cache\n",
        "  caches = (caches, x)\n",
        "\n",
        "  return a, y_pred, caches\n",
        "\n",
        "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
        "  # Retrieve parameters from \"parameters\"\n",
        "  (Wf, bf, Wi, bi, Wc, bc) = (parameters[\"Wf\"], parameters[\"bf\"], parameters[\"Wi\"], parameters[\"bi\"], parameters[\"Wc\"], parameters[\"bc\"])\n",
        "  (Wo, bo, Wy, by) = (parameters[\"Wo\"], parameters[\"bo\"], parameters[\"Wy\"], parameters[\"by\"])\n",
        "\n",
        "  # Retrieve dimensions from shapes of xt, and Wy\n",
        "  n_x, m = xt.shape\n",
        "  n_y, n_a = Wy.shape\n",
        "\n",
        "  # Concatenate a_prev and xt (=3 lines)\n",
        "  concat = np.zeros((n_x + n_a, m))\n",
        "  concat[:n_a, :] = a_prev\n",
        "  concat[n_a:, :] = xt\n",
        "\n",
        "  # Compute values for ft, it, cct, c_next, ot, a_next\n",
        "  # Using the formulas given figure(4) (=6 lines)\n",
        "  ft = sigmoid(np.dot(Wf, concat) + bf)\n",
        "  it = sigmoid(np.dot(Wi, concat) + bi)\n",
        "  cct = np.tanh(np.dot(Wc, concat) + bc)\n",
        "  c_next = c_prev*ft + it*cct\n",
        "  ot = sigmoid(np.dot(Wo, concat) + bo)\n",
        "  a_next = ot*np.tanh(c_next)\n",
        "\n",
        "  # Compute prediction of the LSTM cell (=1 line)\n",
        "  yt_pred = softmax(np.dot(Wy, a_next) + by)\n",
        "\n",
        "  # Store values needed for backward propagation in cache\n",
        "  cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
        "\n",
        "  return a_next, c_next, yt_pred, cache\n",
        "\n",
        "def lstm_forward(x, a0, parameters):\n",
        "  # Initialize \"caches\", which will track the list of all the caches\n",
        "  caches = []\n",
        "\n",
        "  # Retrieve dimensions from shapes of x and parameters[\"Wy\"] (=2 lines)\n",
        "  n_x, m, T_x = x.shape\n",
        "  n_y, n_a = parameters[\"Wy\"].shape\n",
        "\n",
        "  # Initialize \"a\", \"c\" and \"y\" with zeros (=3 lines)\n",
        "  a, c, y = np.zeros((n_a, m, T_x)), np.zeros((n_a, m, T_x)), np.zeros((n_y, m, T_x))\n",
        "\n",
        "  # Initialize a_next and c_next (=2 lines)\n",
        "  a_next = a0\n",
        "  c_next = np.zeros((n_a, m))\n",
        "\n",
        "  # Loop over all time-steps\n",
        "  for t in range(0, T_x):\n",
        "    # Update next hidden state, next memory state, compute the prediction,\n",
        "    # get the cache (=1 line)\n",
        "    a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t], a_next, c_next, parameters)\n",
        "    \n",
        "    # Save the value of the new \"next\" hidden state in a (=1 line)\n",
        "    a[:,:,t] = a_next\n",
        "\n",
        "    # Save the value of the prediction in y (=1 line)\n",
        "    y[:,:,t] = yt\n",
        "\n",
        "    # Save the value of the next cell state (=1 line)\n",
        "    c[:,:,t] = c_next\n",
        "\n",
        "    # Append the cache into caches (=1 line)\n",
        "    caches.append(cache)\n",
        "  \n",
        "  # Store calues needed for backward propagation in cache\n",
        "  caches = (caches, x)\n",
        "\n",
        "  return a, y, c, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB4FkRScuuiY",
        "colab_type": "text"
      },
      "source": [
        "### Backward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXL-xOxthoO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_cell_backward(da_next, cache):\n",
        "  # Retrieve values from cache\n",
        "  (a_next, a_prev, xt, parameters) = cache\n",
        "\n",
        "  # Retrieve values from parameters\n",
        "  (Wax, Waa, Wya, ba, by) = (parameters[\"Wax\"], parameters[\"Waa\"], parameters[\"Wya\"], parameters[\"ba\"], parameters[\"by\"])\n",
        "\n",
        "  # compute the gradient of tanh with respect to a_next (=1 line)\n",
        "  dtanh = 1 - np.power(a_next, 2)\n",
        "\n",
        "  # Compute the gradient of the loss with respect to Wax (=2 lines)\n",
        "  dxt = np.dot(Wax.T, da_next*dtanh)\n",
        "  dWax = np.dot(da_next*dtanh, xt.T)\n",
        "\n",
        "  # Compute the gradient with respect to Waa(=1 line)\n",
        "  da_prev = np.dot(Waa.T, da_next*dtanh)\n",
        "  dWaa = np.dot(da_next*dtanh, a_prev.T)\n",
        "\n",
        "  # compute the gradient with respect to b (=1 line)\n",
        "  dba = np.sum(da_next*dtanh, axis=1, keepdims=True)\n",
        "\n",
        "  # Store the gradients in a python dictionary\n",
        "  gradients = {\"dxt\":dxt, \"da_prev\":da_prev, \"dWax\":dWax, \"dWaa\":dWaa, \"dba\":dba}\n",
        "\n",
        "  return gradients\n",
        "\n",
        "def rnn_backward(da, caches):\n",
        "  # Retrieve values from the first cache (t=1) of caches (=2 lines)\n",
        "  (caches, x) = caches\n",
        "  (a1, a0, x1, parameters) = caches[1]\n",
        "\n",
        "  # Retrieve dimensions from da's and x1's shapes (=2 lines)\n",
        "  n_a, m, T_x = da.shape\n",
        "  n_x, m = x1.shape\n",
        "\n",
        "  # Initialize the gradients with the right sizes (=6 lines)\n",
        "  dx = np.zeros((n_x, m, T_x))\n",
        "  dWax = np.zeros((n_a, n_x))\n",
        "  dWaa = np.zeros((n_a, n_a))\n",
        "  dba = np.zeros((n_a, 1))\n",
        "  db0 = np.zeros((n_a, m))\n",
        "  da_prevt = np.zeros((n_a, m))\n",
        "\n",
        "  # Loop through all the time steps\n",
        "  for t in reversed(range(0, T_x)):\n",
        "    # Compute gradients at time step t. Choose wisely the \"da_next\" and\n",
        "    # the \"cache\" to use in the backward propagation step. (=1 line)\n",
        "    gradients = rnn_cell_backward(da[:,:,t] + da_prevt, caches[t])\n",
        "\n",
        "    # Retrieve derivatives from gradients (=1 line)\n",
        "    (dxt, da_prevt, dWaxt, dWaat, dbat) = (gradients[\"dxt\"], gradients[\"da_prev\"],\n",
        "      gradients[\"dWax\"], gradients[\"dWaa\"], gradients[\"dba\"])\n",
        "\n",
        "    # Increment global derivatives w.r.t parameters by adding\n",
        "    dx[:,:,t] = dxt\n",
        "    dWax += dWaxt\n",
        "    dWaa += dWaat\n",
        "    dba += dbat\n",
        "  \n",
        "  # Set da0 to the gradient of a which has been backpropagated through\n",
        "  # all time-steps (=1 line)\n",
        "  da0 = da_prevt\n",
        "\n",
        "  # Store the gradients in a python dictionary\n",
        "  gradients = {\"dx\":dx, \"da0\":da0, \"dWax\":dWax, \"dWaa\":dWaa, \"dba\":dba}\n",
        "\n",
        "  return gradients\n",
        "\n",
        "def lstm_cell_backward(da_next, dc_next, cache):\n",
        "  # Retrieve information from \"cache\"\n",
        "  (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
        "\n",
        "  # Retrieve dimensions from xt's and a_next's shape (=2 lines)\n",
        "  n_x, m = xt.shape\n",
        "  n_a, m = a_next.shape\n",
        "\n",
        "  # Compute gates related derivatives, you can find their values\n",
        "  # can be found by looking caregully at equations (7) to (10) (=4 lines)\n",
        "  dot = da_next*np.tanh(c_next)\n",
        "  dcct = (da_next*ot*(1-np.power(np.tanh(c_next), 2)) + dc_next)*it\n",
        "  dit = (da_next*ot*(1-np.power(np.tanh(c_next), 2)) + dc_next)*cct\n",
        "  dft = (da_next*ot*(1-np.power(np.tanh(c_next), 2)) + dc_next)*c_prev\n",
        "\n",
        "  # Code equations (7) to (10) (=4 lines)\n",
        "  dit = dit*it*(1-it)\n",
        "  dft = dft*ft*(1-ft)\n",
        "  dot = dot*ot*(1-ot)\n",
        "  dcct = dcct*(1-np.power(cct, 2))\n",
        "\n",
        "  # Compute parameters relatived derivatives. Use equations (11)-(14) (=8 lines)\n",
        "  concat = np.zeros((n_x + n_a, m))\n",
        "  concat[:n_a, :] = a_prev\n",
        "  concat[n_a:, :] = xt\n",
        "  dWf = np.dot(dft, concat.T)\n",
        "  dWi = np.dot(dit, concat.T)\n",
        "  dWc = np.dot(dcct, concat.T)\n",
        "  dWo = np.dot(dot, concat.T)\n",
        "  dbf = np.sum(dft, axis=1, keepdims=True)\n",
        "  dbi = np.sum(dit, axis=1, keepdims=True)\n",
        "  dbc = np.sum(dcct, axis=1, keepdims=True)\n",
        "  dbo = np.sum(dot, axis=1, keepdims=True)\n",
        "\n",
        "  # Compute derivatives w.r.t previous hidden statte, previous memory\n",
        "  # state and input. Use equations (15)-(17). (=3 lines)\n",
        "  da_prevx = np.dot(parameters[\"Wf\"].T, dft) + np.dot(parameters[\"Wo\"].T, dot) + \\\n",
        "    np.dot(parameters[\"Wi\"].T, dit) + np.dot(parameters[\"Wc\"].T, dcct)\n",
        "  da_prev = da_prevx[:n_a, :]\n",
        "  dc_prev = (da_next*ot*(1-np.power(np.tanh(c_next), 2)) + dc_next)*ft\n",
        "  dxt = da_prevx[n_a:, :]\n",
        "\n",
        "  # Save gradients in dictionary\n",
        "  gradients = {\"dxt\":dxt, \"da_prev\":da_prev, \"dc_prev\":dc_prev,\n",
        "               \"dWf\":dWf, \"dbf\":dbf, \"dWi\":dWi, \"dbi\":dbi,\n",
        "               \"dWc\":dWc, \"dbc\":dbc, \"dWo\":dWo, \"dbo\":dbo}\n",
        "  \n",
        "  return gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txh1nValU66S",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wavu3Lkuf4A",
        "colab_type": "text"
      },
      "source": [
        "### Forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCvh8ee2U8-D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0fdd5b51-9613-4b45-9f33-3a8d5d923e2e"
      },
      "source": [
        "np.random.seed(1)\n",
        "xt = np.random.randn(3, 10)\n",
        "a_prev = np.random.randn(5, 10)\n",
        "Waa = np.random.randn(5, 5)\n",
        "Wax = np.random.randn(5, 3)\n",
        "Wya = np.random.randn(2, 5)\n",
        "ba = np.random.randn(5, 1)\n",
        "by = np.random.randn(2, 1)\n",
        "parameters = {\"Waa\":Waa, \"Wax\":Wax, \"Wya\":Wya, \"ba\":ba, \"by\":by}\n",
        "\n",
        "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
        "print(\"a_next[4]\", a_next[4])\n",
        "print(\"a_next.shape\", a_next.shape)\n",
        "print(\"yt_pred[1]\", yt_pred[1])\n",
        "print(\"yt_pred.shape\", yt_pred.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_next[4] [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
            " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
            "a_next.shape (5, 10)\n",
            "yt_pred[1] [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n",
            " 0.36920224 0.9966312  0.9982559  0.17746526]\n",
            "yt_pred.shape (2, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu9HPfR6V1mJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5f100155-891a-4645-9c61-43fddfe10900"
      },
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(3, 10, 4)\n",
        "a0 = np.random.randn(5, 10)\n",
        "Waa = np.random.randn(5, 5)\n",
        "Wax = np.random.randn(5, 3)\n",
        "Wya = np.random.randn(2, 5)\n",
        "ba = np.random.randn(5, 1)\n",
        "by = np.random.randn(2, 1)\n",
        "parameters = {\"Waa\":Waa, \"Wax\":Wax, \"Wya\":Wya, \"ba\":ba, \"by\":by}\n",
        "\n",
        "a, y_pred, caches = rnn_forward(x, a0, parameters)\n",
        "print(\"a[4][1]\", a[4][1])\n",
        "print(\"a.shape\", a.shape)\n",
        "print(\"y_pred[1]\", y_pred[1][3])\n",
        "print(\"y_pred.shape\", y_pred.shape)\n",
        "print(\"caches[1]\", caches[1][1][3])\n",
        "print(\"len(caches)\", len(caches))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a[4][1] [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
            "a.shape (5, 10, 4)\n",
            "y_pred[1] [0.79560373 0.86224861 0.11118257 0.81515947]\n",
            "y_pred.shape (2, 10, 4)\n",
            "caches[1] [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
            "len(caches) 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ0uQSTQYPFt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e3f1ff88-3fba-438e-dee6-638ca35d2f0b"
      },
      "source": [
        "np.random.seed(1)\n",
        "xt = np.random.randn(3, 10)\n",
        "a_prev = np.random.randn(5, 10)\n",
        "c_prev = np.random.randn(5, 10)\n",
        "Wf = np.random.randn(5, 5 + 3)\n",
        "bf = np.random.randn(5, 1)\n",
        "Wi = np.random.randn(5, 5 + 3)\n",
        "bi = np.random.randn(5, 1)\n",
        "Wo = np.random.randn(5, 5 + 3)\n",
        "bo = np.random.randn(5, 1)\n",
        "Wc = np.random.randn(5, 5 + 3)\n",
        "bc = np.random.randn(5, 1)\n",
        "Wy = np.random.randn(2, 5)\n",
        "by = np.random.randn(2, 1)\n",
        "parameters = {\"Wf\":Wf, \"Wi\":Wi, \"Wo\":Wo, \"Wc\":Wc, \"Wy\":Wy,\n",
        "              \"bf\":bf, \"bi\":bi, \"bo\":bo, \"bc\":bc, \"by\":by}\n",
        "\n",
        "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
        "print(\"a_next[4] = \", a_next[4])\n",
        "print(\"a_next.shape = \", a_next.shape)\n",
        "print(\"c_next[2] = \", c_next[2])\n",
        "print(\"c_next.shape = \", c_next.shape)\n",
        "print(\"yt[1] = \", yt[1])\n",
        "print(\"yt.shape = \", yt.shape)\n",
        "print(\"cache[1][3] = \", cache[1][3])\n",
        "print(\"len(cache) = \", len(cache))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_next[4] =  [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n",
            "  0.76566531  0.34631421 -0.00215674  0.43827275]\n",
            "a_next.shape =  (5, 10)\n",
            "c_next[2] =  [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n",
            "  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n",
            "c_next.shape =  (5, 10)\n",
            "yt[1] =  [0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381\n",
            " 0.00943007 0.12666353 0.39380172 0.07828381]\n",
            "yt.shape =  (2, 10)\n",
            "cache[1][3] =  [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n",
            "  0.07651101 -1.03752894  1.41219977 -0.37647422]\n",
            "len(cache) =  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uGM-BOibCdp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "cbf04901-2b02-4753-de48-5c284334d3ef"
      },
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(3, 10, 7)\n",
        "a0 = np.random.randn(5, 10)\n",
        "Wf = np.random.randn(5, 5 + 3)\n",
        "bf = np.random.randn(5, 1)\n",
        "Wi = np.random.randn(5, 5 + 3)\n",
        "bi = np.random.randn(5, 1)\n",
        "Wo = np.random.randn(5, 5 + 3)\n",
        "bo = np.random.randn(5, 1)\n",
        "Wc = np.random.randn(5, 5 + 3)\n",
        "bc = np.random.randn(5, 1)\n",
        "Wy = np.random.randn(2, 5)\n",
        "by = np.random.randn(2, 1)\n",
        "parameters = {\"Wf\":Wf, \"Wi\":Wi, \"Wo\":Wo, \"Wc\":Wc, \"Wy\":Wy,\n",
        "              \"bf\":bf, \"bi\":bi, \"bo\":bo, \"bc\":bc, \"by\":by}\n",
        "\n",
        "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
        "print(\"a[4][3][6] = \", a[4][3][6])\n",
        "print(\"a.shape = \", a.shape)\n",
        "print(\"y[1][4][3] = \", y[1][4][3])\n",
        "print(\"y.shape = \", yt.shape)\n",
        "print(\"caches[1][1][1] = \", caches[1][1][1])\n",
        "print(\"c[1][2][1] = \", c[1][2][1])\n",
        "print(\"len(cache) = \", len(cache))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a[4][3][6] =  0.17211776753291672\n",
            "a.shape =  (5, 10, 7)\n",
            "y[1][4][3] =  0.9508734618501101\n",
            "y.shape =  (2, 10)\n",
            "caches[1][1][1] =  [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
            "  0.41005165]\n",
            "c[1][2][1] =  -0.8555449167181981\n",
            "len(cache) =  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8paE026Oulzw",
        "colab_type": "text"
      },
      "source": [
        "### Backward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2k2nShWirnI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a704071f-f5c0-4e61-e082-b287cea0c2a3"
      },
      "source": [
        "np.random.seed(1)\n",
        "xt = np.random.randn(3, 10)\n",
        "a_prev = np.random.randn(5, 10)\n",
        "Wax = np.random.randn(5, 3)\n",
        "Waa = np.random.randn(5, 5)\n",
        "Wya = np.random.randn(2, 5)\n",
        "ba = np.random.randn(5, 1)\n",
        "by = np.random.randn(2, 1)\n",
        "parameters = {\"Waa\":Waa, \"Wax\":Wax, \"Wya\":Wya, \"ba\":ba, \"by\":by}\n",
        "\n",
        "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
        "\n",
        "da_next = np.random.randn(5, 10)\n",
        "gradients = rnn_cell_backward(da_next, cache)\n",
        "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
        "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
        "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
        "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
        "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
        "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradients[\"dxt\"][1][2] = -1.3872130506020925\n",
            "gradients[\"dxt\"].shape = (3, 10)\n",
            "gradients[\"da_prev\"][2][3] = -0.15239949377395495\n",
            "gradients[\"da_prev\"].shape = (5, 10)\n",
            "gradients[\"dWax\"][3][1] = 0.4107728249354584\n",
            "gradients[\"dWax\"].shape = (5, 3)\n",
            "gradients[\"dWaa\"][1][2] = 1.1503450668497135\n",
            "gradients[\"dWaa\"].shape = (5, 5)\n",
            "gradients[\"dba\"][4] = [0.20023491]\n",
            "gradients[\"dba\"].shape = (5, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCxB4_HbmaTR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ecebe3bb-245d-4e65-c394-0222ea53d018"
      },
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(3, 10, 4)\n",
        "a0 = np.random.randn(5, 10)\n",
        "Wax = np.random.randn(5, 3)\n",
        "Waa = np.random.randn(5, 5)\n",
        "Wya = np.random.randn(2, 5)\n",
        "ba = np.random.randn(5, 1)\n",
        "by = np.random.randn(2, 1)\n",
        "parameters = {\"Waa\":Waa, \"Wax\":Wax, \"Wya\":Wya, \"ba\":ba, \"by\":by}\n",
        "a, y, caches = rnn_forward(x, a0, parameters)\n",
        "\n",
        "da = np.random.randn(5, 10, 4)\n",
        "gradients = rnn_backward(da, caches)\n",
        "\n",
        "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
        "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
        "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
        "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
        "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
        "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradients[\"dx\"][1][2] = [-2.07101689 -0.59255627  0.02466855  0.01483317]\n",
            "gradients[\"dx\"].shape = (3, 10, 4)\n",
            "gradients[\"da0\"][2][3] = -0.31494237512664996\n",
            "gradients[\"da0\"].shape = (5, 10)\n",
            "gradients[\"dWax\"][3][1] = 11.264104496527777\n",
            "gradients[\"dWax\"].shape = (5, 3)\n",
            "gradients[\"dWaa\"][1][2] = 2.303333126579893\n",
            "gradients[\"dWaa\"].shape = (5, 5)\n",
            "gradients[\"dba\"][4] = [-0.74747722]\n",
            "gradients[\"dba\"].shape = (5, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9sy5PATqYgG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "b8ba4e3b-af1a-4bce-fe0d-0114c4ce9831"
      },
      "source": [
        "np.random.seed(1)\n",
        "xt = np.random.randn(3, 10)\n",
        "a_prev = np.random.randn(5, 10)\n",
        "c_prev = np.random.randn(5, 10)\n",
        "Wf = np.random.randn(5, 5 + 3)\n",
        "bf = np.random.randn(5, 1)\n",
        "Wi = np.random.randn(5, 5 + 3)\n",
        "bi = np.random.randn(5, 1)\n",
        "Wo = np.random.randn(5, 5 + 3)\n",
        "bo = np.random.randn(5, 1)\n",
        "Wc = np.random.randn(5, 5 + 3)\n",
        "bc = np.random.randn(5, 1)\n",
        "Wy = np.random.randn(2, 5)\n",
        "by = np.random.randn(2, 1)\n",
        "parameters = {\"Wf\":Wf, \"Wi\":Wi, \"Wo\":Wo, \"Wc\":Wc, \"Wy\":Wy,\n",
        "              \"bf\":bf, \"bi\":bi, \"bo\":bo, \"bc\":bc, \"by\":by}\n",
        "\n",
        "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
        "\n",
        "da_next = np.random.randn(5, 10)\n",
        "dc_next = np.random.randn(5, 10)\n",
        "gradients = lstm_cell_backward(da_next, dc_next, cache)\n",
        "print(\"gradients[\\\"dxt\\\"][1][2] = \", gradients[\"dxt\"][1][2])\n",
        "print(\"gradients[\\\"dxt\\\"].shape = \", gradients[\"dxt\"].shape)\n",
        "print(\"gradients[\\\"da_prev\\\"][2][3] = \", gradients[\"da_prev\"][2][3])\n",
        "print(\"gradients[\\\"da_prev\\\"].shape = \", gradients[\"da_prev\"].shape)\n",
        "print(\"gradients[\\\"dc_prev\\\"][2][3] = \", gradients[\"dc_prev\"][2][3])\n",
        "print(\"gradients[\\\"dc_prev\\\"].shape = \", gradients[\"dc_prev\"].shape)\n",
        "print(\"gradients[\\\"dWf\\\"][3][1] = \", gradients[\"dWf\"][3][1])\n",
        "print(\"gradients[\\\"dWf\\\"].shape = \", gradients[\"dWf\"].shape)\n",
        "print(\"gradients[\\\"dWi\\\"][1][2] = \", gradients[\"dWi\"][1][2])\n",
        "print(\"gradients[\\\"dWi\\\"].shape = \", gradients[\"dWi\"].shape)\n",
        "print(\"gradients[\\\"dWc\\\"][3][1] = \", gradients[\"dWc\"][3][1])\n",
        "print(\"gradients[\\\"dWc\\\"].shape = \", gradients[\"dWc\"].shape)\n",
        "print(\"gradients[\\\"dWo\\\"][1][2] = \", gradients[\"dWo\"][1][2])\n",
        "print(\"gradients[\\\"dWo\\\"].shape = \", gradients[\"dWo\"].shape)\n",
        "print(\"gradients[\\\"dbf\\\"][4] = \", gradients[\"dbf\"][4])\n",
        "print(\"gradients[\\\"dbf\\\"].shape = \", gradients[\"dbf\"].shape)\n",
        "print(\"gradients[\\\"dbi\\\"][4] = \", gradients[\"dbi\"][4])\n",
        "print(\"gradients[\\\"dbi\\\"].shape = \", gradients[\"dbi\"].shape)\n",
        "print(\"gradients[\\\"dbc\\\"][4] = \", gradients[\"dbc\"][4])\n",
        "print(\"gradients[\\\"dbc\\\"].shape = \", gradients[\"dbc\"].shape)\n",
        "print(\"gradients[\\\"dbo\\\"][4] = \", gradients[\"dbo\"][4])\n",
        "print(\"gradients[\\\"dbo\\\"].shape = \", gradients[\"dbo\"].shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradients[\"dxt\"][1][2] =  3.230559115109188\n",
            "gradients[\"dxt\"].shape =  (3, 10)\n",
            "gradients[\"da_prev\"][2][3] =  -0.06396214197109235\n",
            "gradients[\"da_prev\"].shape =  (5, 10)\n",
            "gradients[\"dc_prev\"][2][3] =  0.7975220387970015\n",
            "gradients[\"dc_prev\"].shape =  (5, 10)\n",
            "gradients[\"dWf\"][3][1] =  -0.14795483816449675\n",
            "gradients[\"dWf\"].shape =  (5, 8)\n",
            "gradients[\"dWi\"][1][2] =  1.0574980552259903\n",
            "gradients[\"dWi\"].shape =  (5, 8)\n",
            "gradients[\"dWc\"][3][1] =  2.304562163687667\n",
            "gradients[\"dWc\"].shape =  (5, 8)\n",
            "gradients[\"dWo\"][1][2] =  0.3313115952892109\n",
            "gradients[\"dWo\"].shape =  (5, 8)\n",
            "gradients[\"dbf\"][4] =  [0.18864637]\n",
            "gradients[\"dbf\"].shape =  (5, 1)\n",
            "gradients[\"dbi\"][4] =  [-0.40142491]\n",
            "gradients[\"dbi\"].shape =  (5, 1)\n",
            "gradients[\"dbc\"][4] =  [0.25587763]\n",
            "gradients[\"dbc\"].shape =  (5, 1)\n",
            "gradients[\"dbo\"][4] =  [0.13893342]\n",
            "gradients[\"dbo\"].shape =  (5, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}